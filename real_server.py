#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå®å½©ç¥¨æ•°æ®æœåŠ¡å™¨
ä»å¤šä¸ªå®˜æ–¹æ•°æ®æºè·å–çœŸå®çš„ç¦å½©3Då’ŒåŒè‰²çƒå¼€å¥–æ•°æ®
"""

from flask import Flask, jsonify, request, make_response
from flask_cors import CORS
import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime, timedelta
import time
import logging
import random
from urllib.parse import urljoin, urlparse

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
# å…è®¸ /api/* è·¨åŸŸï¼Œç¡®ä¿åœ¨é”™è¯¯å“åº”ä¹Ÿè¿”å› CORS å¤´
CORS(app, resources={r"/api/*": {"origins": "*"}})

@app.after_request
def add_cors_headers(resp):
    try:
        resp.headers['Access-Control-Allow-Origin'] = '*'
        resp.headers['Access-Control-Allow-Methods'] = 'GET,POST,OPTIONS'
        resp.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'
    except Exception:
        pass
    return resp

@app.route('/api/<path:subpath>', methods=['OPTIONS'])
def api_cors_preflight(subpath):
    resp = make_response('', 200)
    resp.headers['Access-Control-Allow-Origin'] = '*'
    resp.headers['Access-Control-Allow-Methods'] = 'GET,POST,OPTIONS'
    resp.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'
    return resp

class RealLotteryDataScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # æ•°æ®ç¼“å­˜
        self.cache = {}
        self.cache_timeout = 300  # 5åˆ†é’Ÿç¼“å­˜
    
    def get_fc3d_data(self, limit=300):
        """è·å–ç¦å½©3Då†å²æ•°æ®"""
        cache_key = f"fc3d_{limit}"
        if self._is_cache_valid(cache_key):
            logger.info("ä½¿ç”¨ç¼“å­˜çš„ç¦å½©3Dæ•°æ®")
            return self.cache[cache_key]['data']
        
        try:
            # å°è¯•å¤šä¸ªçœŸå®æ•°æ®æºï¼ˆæ— æ¨¡æ‹Ÿæ•°æ®å›é€€ï¼‰
            data_sources = [
                self._scrape_fc3d_from_500_new(),
                self._scrape_fc3d_from_cwl_new(),
                self._scrape_fc3d_from_sina_new(),
                self._scrape_fc3d_from_163()
            ]
            
            for i, data in enumerate(data_sources):
                if data and len(data) > 0:
                    logger.info(f"æˆåŠŸä»æ•°æ®æº{i+1}è·å–ç¦å½©3Dæ•°æ®ï¼Œå…±{len(data)}æœŸ")
                    self._cache_data(cache_key, data)
                    return data[:limit]
            
            # æ‰€æœ‰æºå¤±è´¥ï¼Œè¿”å›ç©º
            logger.warning("æ‰€æœ‰ç¦å½©3Dæ•°æ®æºéƒ½å¤±è´¥")
            return []
            
        except Exception as e:
            logger.error(f"è·å–ç¦å½©3Dæ•°æ®å¤±è´¥: {e}")
            return []
    
    def get_ssq_data(self, limit=300):
        """è·å–åŒè‰²çƒå†å²æ•°æ®"""
        cache_key = f"ssq_{limit}"
        if self._is_cache_valid(cache_key):
            logger.info("ä½¿ç”¨ç¼“å­˜çš„åŒè‰²çƒæ•°æ®")
            return self.cache[cache_key]['data']
        
        try:
            # å°è¯•å¤šä¸ªæ•°æ®æº
            data_sources = [
                self._scrape_ssq_from_500_new(),
                self._scrape_ssq_from_cwl_new(),
                self._scrape_ssq_from_sina_new(),
                self._scrape_ssq_from_163()
            ]
            
            for i, data in enumerate(data_sources):
                if data and len(data) > 0:
                    logger.info(f"æˆåŠŸä»æ•°æ®æº{i+1}è·å–åŒè‰²çƒæ•°æ®ï¼Œå…±{len(data)}æœŸ")
                    # æ›´æ–°æœ€æ–°å¼€å¥–æ—¥æœŸ
                    data = self._update_latest_ssq_date(data)
                    self._cache_data(cache_key, data)
                    return data[:limit]
            
            # å¦‚æœæ‰€æœ‰æºéƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            logger.warning("æ‰€æœ‰åŒè‰²çƒæ•°æ®æºéƒ½å¤±è´¥")
            return []
            
        except Exception as e:
            logger.error(f"è·å–åŒè‰²çƒæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _update_latest_ssq_date(self, data):
        """æ›´æ–°åŒè‰²çƒä¸‹ä¸€æœŸå¼€å¥–æ—¥æœŸ"""
        if not data:
            return data
        
        try:
            # è·å–å½“å‰æ—¥æœŸ
            from datetime import datetime, timedelta
            today = datetime.now()
            
            # åŒè‰²çƒå¼€å¥–æ—¶é—´ï¼šå‘¨äºŒã€å››ã€æ—¥ 21:15
            # è®¡ç®—ä¸‹ä¸€æœŸå¼€å¥–æ—¥æœŸ
            days_since_monday = today.weekday()  # 0=å‘¨ä¸€, 1=å‘¨äºŒ, 2=å‘¨ä¸‰, 3=å‘¨å››, 4=å‘¨äº”, 5=å‘¨å…­, 6=å‘¨æ—¥
            
            if days_since_monday == 0:  # å‘¨ä¸€
                next_draw = today + timedelta(days=1)  # æ˜å¤©å‘¨äºŒ
            elif days_since_monday == 1:  # å‘¨äºŒ
                next_draw = today + timedelta(days=2)  # åå¤©å‘¨å››
            elif days_since_monday == 2:  # å‘¨ä¸‰
                next_draw = today + timedelta(days=1)  # æ˜å¤©å‘¨å››
            elif days_since_monday == 3:  # å‘¨å››
                next_draw = today + timedelta(days=3)  # åå¤©å‘¨æ—¥
            elif days_since_monday == 4:  # å‘¨äº”
                next_draw = today + timedelta(days=2)  # åå¤©å‘¨æ—¥
            elif days_since_monday == 5:  # å‘¨å…­
                next_draw = today + timedelta(days=1)  # æ˜å¤©å‘¨æ—¥
            else:  # å‘¨æ—¥
                next_draw = today + timedelta(days=2)  # åå¤©å‘¨äºŒ
            
            # æ›´æ–°ç¬¬ä¸€æ¡æ•°æ®çš„æ—¥æœŸä¸ºä¸‹ä¸€æœŸå¼€å¥–æ—¥æœŸ
            if data and len(data) > 0:
                # æ ¼å¼åŒ–æ—¥æœŸ
                weekday_names = ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'æ—¥']
                weekday = weekday_names[next_draw.weekday()]
                next_date = f"{next_draw.strftime('%Y-%m-%d')}({weekday})"
                
                # æ›´æ–°æœŸå·ï¼ˆå‡è®¾æ˜¯è¿ç»­é€’å¢çš„ï¼‰
                if 'period' in data[0]:
                    try:
                        current_period = int(data[0]['period'])
                        next_period = str(current_period + 1)
                        data[0]['period'] = next_period
                    except:
                        pass
                
                data[0]['date'] = next_date
                logger.info(f"æ›´æ–°åŒè‰²çƒä¸‹ä¸€æœŸå¼€å¥–æ—¥æœŸä¸º: {next_date}")
            
            return data
            
        except Exception as e:
            logger.error(f"æ›´æ–°åŒè‰²çƒå¼€å¥–æ—¥æœŸå¤±è´¥: {e}")
            return data
    
    def _update_latest_fc3d_date(self, data):
        """æ›´æ–°ç¦å½©3Dä¸‹ä¸€æœŸå¼€å¥–æ—¥æœŸ"""
        if not data:
            return data
        
        try:
            # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´
            from datetime import datetime, timedelta
            now = datetime.now()
            
            # ç¦å½©3Då¼€å¥–æ—¶é—´ï¼šæ¯å¤©21:15
            # ä»¥21:15ä¸ºç•Œï¼Œè¿‡äº†21:15å°±æ˜¾ç¤ºç¬¬äºŒå¤©çš„æ—¥æœŸ
            if now.hour >= 21 and now.minute >= 15:
                # å·²ç»è¿‡äº†21:15ï¼Œä¸‹ä¸€æœŸæ˜¯æ˜å¤©
                next_draw = now + timedelta(days=1)
            else:
                # è¿˜æ²¡åˆ°21:15ï¼Œä¸‹ä¸€æœŸæ˜¯ä»Šå¤©
                next_draw = now
            
            # æ›´æ–°ç¬¬ä¸€æ¡æ•°æ®çš„æ—¥æœŸä¸ºä¸‹ä¸€æœŸå¼€å¥–æ—¥æœŸ
            if data and len(data) > 0:
                # æ ¼å¼åŒ–æ—¥æœŸ
                next_date = next_draw.strftime('%Y-%m-%d')
                
                # æ›´æ–°æœŸå·ï¼ˆå‡è®¾æ˜¯è¿ç»­é€’å¢çš„ï¼‰
                if 'period' in data[0]:
                    try:
                        current_period = int(data[0]['period'])
                        next_period = str(current_period + 1)
                        data[0]['period'] = next_period
                    except:
                        pass
                
                data[0]['date'] = next_date
                logger.info(f"æ›´æ–°ç¦å½©3Dä¸‹ä¸€æœŸå¼€å¥–æ—¥æœŸä¸º: {next_date} (å½“å‰æ—¶é—´: {now.strftime('%H:%M')})")
            
            return data
            
        except Exception as e:
            logger.error(f"æ›´æ–°ç¦å½©3Då¼€å¥–æ—¥æœŸå¤±è´¥: {e}")
            return data
    
    def _scrape_fc3d_from_500_new(self):
        """ä»500.comè·å–ç¦å½©3Dæ•°æ®ï¼ˆæ–°ç‰ˆæœ¬ï¼‰"""
        try:
            # å°è¯•æ–°çš„URLæ ¼å¼
            urls = [
                "https://datachart.500.com/fc3d/history/newinc/history.php",
                "https://datachart.500.com/fc3d/history/history.shtml",
                "https://www.500.com/fc3d/history/"
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=15)
                    if response.status_code == 200:
                        response.encoding = 'gb2312'
                        soup = BeautifulSoup(response.text, 'html.parser')
                        data = self._parse_fc3d_from_html(soup)
                        if data:
                            return data
                except Exception as e:
                    logger.debug(f"500.com URL {url} å¤±è´¥: {e}")
                    continue
            
            return []
            
        except Exception as e:
            logger.error(f"ä»500.comè·å–ç¦å½©3Dæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _scrape_ssq_from_500_new(self):
        """ä»500.comè·å–åŒè‰²çƒæ•°æ®ï¼ˆæ–°ç‰ˆæœ¬ï¼‰"""
        try:
            # å°è¯•æ–°çš„URLæ ¼å¼
            urls = [
                "https://datachart.500.com/ssq/history/newinc/history.php",
                "https://datachart.500.com/ssq/history/history.shtml",
                "https://www.500.com/ssq/history/"
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=15)
                    if response.status_code == 200:
                        response.encoding = 'gb2312'
                        soup = BeautifulSoup(response.text, 'html.parser')
                        data = self._parse_ssq_from_html(soup)
                        if data:
                            return data
                except Exception as e:
                    logger.debug(f"500.com URL {url} å¤±è´¥: {e}")
                    continue
            
            return []
            
        except Exception as e:
            logger.error(f"ä»500.comè·å–åŒè‰²çƒæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _scrape_fc3d_from_cwl_new(self):
        """ä»ä¸­å›½ç¦å½©ç½‘è·å–ç¦å½©3Dæ•°æ®ï¼ˆæ–°ç‰ˆæœ¬ï¼‰"""
        try:
            # ä½¿ç”¨æ–°çš„APIæ¥å£
            url = "https://www.cwl.gov.cn/cwl_admin/front/cwlkj/search/kjxx/findDrawNotice"
            params = {
                'name': 'fc3d',
                'issueCount': '300',
                'issueStart': '',
                'issueEnd': '',
                'dayStart': '',
                'dayEnd': ''
            }
            
            response = self.session.get(url, params=params, timeout=15)
            if response.status_code == 200:
                result = response.json()
                if result.get('state') == 0 and result.get('result'):
                    data = []
                    for item in result['result']:
                        data.append({
                            'period': item.get('code', ''),
                            'date': item.get('date', ''),
                            'number': item.get('red', '')
                        })
                    return data
            
            return []
            
        except Exception as e:
            logger.error(f"ä»ä¸­å›½ç¦å½©ç½‘è·å–ç¦å½©3Dæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _scrape_ssq_from_cwl_new(self):
        """ä»ä¸­å›½ç¦å½©ç½‘è·å–åŒè‰²çƒæ•°æ®ï¼ˆæ–°ç‰ˆæœ¬ï¼‰"""
        try:
            # ä½¿ç”¨æ–°çš„APIæ¥å£
            url = "https://www.cwl.gov.cn/cwl_admin/front/cwlkj/search/kjxx/findDrawNotice"
            params = {
                'name': 'ssq',
                'issueCount': '300',
                'issueStart': '',
                'issueEnd': '',
                'dayStart': '',
                'dayEnd': ''
            }
            
            response = self.session.get(url, params=params, timeout=15)
            if response.status_code == 200:
                result = response.json()
                if result.get('state') == 0 and result.get('result'):
                    data = []
                    for item in result['result']:
                        data.append({
                            'period': item.get('code', ''),
                            'date': item.get('date', ''),
                            'redBalls': item.get('red', ''),
                            'blueBall': item.get('blue', '')
                        })
                    return data
            
            return []
            
        except Exception as e:
            logger.error(f"ä»ä¸­å›½ç¦å½©ç½‘è·å–åŒè‰²çƒæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _scrape_fc3d_from_sina_new(self):
        """ä»æ–°æµªè·å–ç¦å½©3Dæ•°æ®ï¼ˆæ–°ç‰ˆæœ¬ï¼‰"""
        try:
            urls = [
                "https://match.lottery.sina.com.cn/lotto/pc_zst/index?lottoType=fc3d&actionType=chzs",
                "https://sports.sina.com.cn/lottery/fc3d/history.shtml"
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=15)
                    if response.status_code == 200:
                        response.encoding = 'utf-8'
                        soup = BeautifulSoup(response.text, 'html.parser')
                        data = self._parse_fc3d_from_html(soup)
                        if data:
                            return data
                except Exception as e:
                    logger.debug(f"æ–°æµª URL {url} å¤±è´¥: {e}")
                    continue
            
            return []
            
        except Exception as e:
            logger.error(f"ä»æ–°æµªè·å–ç¦å½©3Dæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _scrape_ssq_from_sina_new(self):
        """ä»æ–°æµªè·å–åŒè‰²çƒæ•°æ®ï¼ˆæ–°ç‰ˆæœ¬ï¼‰"""
        try:
            urls = [
                "https://match.lottery.sina.com.cn/lotto/pc_zst/index?lottoType=ssq&actionType=chzs",
                "https://sports.sina.com.cn/lottery/ssq/history.shtml"
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=15)
                    if response.status_code == 200:
                        response.encoding = 'utf-8'
                        soup = BeautifulSoup(response.text, 'html.parser')
                        data = self._parse_ssq_from_html(soup)
                        if data:
                            return data
                except Exception as e:
                    logger.debug(f"æ–°æµª URL {url} å¤±è´¥: {e}")
                    continue
            
            return []
            
        except Exception as e:
            logger.error(f"ä»æ–°æµªè·å–åŒè‰²çƒæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _scrape_fc3d_from_163(self):
        """ä»ç½‘æ˜“è·å–ç¦å½©3Dæ•°æ®"""
        try:
            url = "https://caipiao.163.com/award/fc3d/"
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                data = self._parse_fc3d_from_html(soup)
                if data:
                    return data
            return []
            
        except Exception as e:
            logger.error(f"ä»ç½‘æ˜“è·å–ç¦å½©3Dæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _scrape_ssq_from_163(self):
        """ä»ç½‘æ˜“è·å–åŒè‰²çƒæ•°æ®"""
        try:
            url = "https://caipiao.163.com/award/ssq/"
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                data = self._parse_ssq_from_html(soup)
                if data:
                    return data
            return []
            
        except Exception as e:
            logger.error(f"ä»ç½‘æ˜“è·å–åŒè‰²çƒæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _parse_fc3d_from_html(self, soup):
        """ä»HTMLä¸­è§£æç¦å½©3Dæ•°æ®"""
        data = []
        
        # å°è¯•å¤šç§è¡¨æ ¼é€‰æ‹©å™¨
        selectors = [
            'table#tdata',
            'table.tb_0',
            'table.history-table',
            'table[class*="table"]',
            'table'
        ]
        
        for selector in selectors:
            table = soup.select_one(selector)
            if table:
                rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 4:
                        period = cells[0].get_text(strip=True)
                        date = cells[1].get_text(strip=True)
                        number = cells[2].get_text(strip=True)
                        
                        # æ¸…ç†æ•°æ®
                        number = re.sub(r'[^\d]', '', number)
                        
                        if period and date and number and len(number) == 3:
                            data.append({
                                'period': period,
                                'date': date,
                                'number': number
                            })
                
                if data:
                    break
        
        return data
    
    def _parse_ssq_from_html(self, soup):
        """ä»HTMLä¸­è§£æåŒè‰²çƒæ•°æ®"""
        data = []
        
        # å°è¯•å¤šç§è¡¨æ ¼é€‰æ‹©å™¨
        selectors = [
            'table#tdata',
            'table.tb_0',
            'table.history-table',
            'table[class*="table"]',
            'table'
        ]
        
        for selector in selectors:
            table = soup.select_one(selector)
            if table:
                rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 4:
                        period = cells[0].get_text(strip=True)
                        date = cells[1].get_text(strip=True)
                        red_balls = cells[2].get_text(strip=True)
                        blue_ball = cells[3].get_text(strip=True)
                        
                        # æ¸…ç†çº¢çƒæ•°æ®
                        red_balls = re.sub(r'[^\d,]', '', red_balls)
                        red_list = [x.strip() for x in red_balls.split(',') if x.strip()]
                        
                        # æ¸…ç†è“çƒæ•°æ®
                        blue_ball = re.sub(r'[^\d]', '', blue_ball)
                        
                        if period and date and len(red_list) == 6 and blue_ball:
                            data.append({
                                'period': period,
                                'date': date,
                                'redBalls': red_list,
                                'blueBall': blue_ball
                            })
                
                if data:
                    break
        
        return data
    
    def _is_cache_valid(self, key):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if key not in self.cache:
            return False
        
        cache_time = self.cache[key]['timestamp']
        return time.time() - cache_time < self.cache_timeout
    
    def _cache_data(self, key, data):
        """ç¼“å­˜æ•°æ®"""
        self.cache[key] = {
            'data': data,
            'timestamp': time.time()
        }
    
    def _generate_realistic_fc3d_data(self, count=300):
        """ç”ŸæˆçœŸå®çš„ç¦å½©3Dæµ‹è¯•æ•°æ®"""
        data = []
        today = datetime.now()
        
        for i in range(count):
            # ç”ŸæˆæœŸå·ï¼ˆ2024å¹´æ ¼å¼ï¼‰
            period = f"2024{str(1000 - i).zfill(3)}"
            
            # ç”Ÿæˆæ—¥æœŸï¼ˆå€’æ¨ï¼‰
            date = (today - timedelta(days=i)).strftime('%Y-%m-%d')
            
            # ç”Ÿæˆå·ç ï¼ˆåŸºäºçœŸå®ç»Ÿè®¡è§„å¾‹ï¼‰
            number = self._generate_realistic_fc3d_number()
            
            data.append({
                'period': period,
                'date': date,
                'number': number
            })
        
        return data
    
    def _generate_realistic_fc3d_number(self):
        """ç”ŸæˆçœŸå®çš„ç¦å½©3Då·ç """
        # åŸºäºçœŸå®ç»Ÿè®¡çš„æƒé‡
        weights = [0.105, 0.108, 0.102, 0.095, 0.098, 0.100, 0.102, 0.105, 0.108, 0.107]
        
        number = ''
        for _ in range(3):
            rand = random.random()
            cumulative = 0
            for i, weight in enumerate(weights):
                cumulative += weight
                if rand <= cumulative:
                    number += str(i)
                    break
        
        # é¿å…å…¨ç›¸åŒæ•°å­—
        if number[0] == number[1] == number[2]:
            number = number[:2] + str(random.randint(0, 9))
        
        return number

# åˆ›å»ºçˆ¬è™«å®ä¾‹
scraper = RealLotteryDataScraper()

@app.route('/api/fc3d', methods=['GET'])
def get_fc3d():
    """è·å–ç¦å½©3Dæ•°æ®API"""
    try:
        limit = request.args.get('limit', 300, type=int)
        data = scraper.get_fc3d_data(limit)
        
        if data:
            return jsonify({
                'success': True,
                'data': data,
                'count': len(data),
                'source': 'çœŸå®å¼€å¥–æ•°æ®'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'æ— æ³•è·å–ç¦å½©3Dæ•°æ®',
                'data': []
            }), 500
            
    except Exception as e:
        logger.error(f"ç¦å½©3D APIé”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'message': str(e),
            'data': []
        }), 500

@app.route('/api/ssq', methods=['GET'])
def get_ssq():
    """è·å–åŒè‰²çƒæ•°æ®API"""
    try:
        limit = request.args.get('limit', 300, type=int)
        data = scraper.get_ssq_data(limit)
        
        if data:
            return jsonify({
                'success': True,
                'data': data,
                'count': len(data),
                'source': 'çœŸå®å¼€å¥–æ•°æ®'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'æ— æ³•è·å–åŒè‰²çƒæ•°æ®',
                'data': []
            }), 500
            
    except Exception as e:
        logger.error(f"åŒè‰²çƒ APIé”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'message': str(e),
            'data': []
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥API"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'message': 'çœŸå®å½©ç¥¨æ•°æ®æœåŠ¡å™¨è¿è¡Œæ­£å¸¸',
        'cache_size': len(scraper.cache)
    })

@app.route('/api/clear_cache', methods=['POST'])
def clear_cache():
    """æ¸…é™¤ç¼“å­˜API"""
    scraper.cache.clear()
    return jsonify({
        'success': True,
        'message': 'ç¼“å­˜å·²æ¸…é™¤'
    })

if __name__ == '__main__':
    import os
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--port', type=int, default=int(os.environ.get('PORT', 5000)))
    args = parser.parse_args()
    port = args.port

    print("ğŸš€ å¯åŠ¨çœŸå®å½©ç¥¨æ•°æ®æœåŠ¡å™¨...")
    print("ğŸ“Š æ”¯æŒä»å¤šä¸ªå®˜æ–¹æ•°æ®æºè·å–çœŸå®å¼€å¥–æ•°æ®")
    print("ğŸŒ APIåœ°å€:")
    print(f"   - ç¦å½©3D: http://localhost:{port}/api/fc3d")
    print(f"   - åŒè‰²çƒ: http://localhost:{port}/api/ssq")
    print(f"   - å¥åº·æ£€æŸ¥: http://localhost:{port}/api/health")
    print(f"   - æ¸…é™¤ç¼“å­˜: http://localhost:{port}/api/clear_cache")
    
    app.run(host='0.0.0.0', port=port, debug=True)
